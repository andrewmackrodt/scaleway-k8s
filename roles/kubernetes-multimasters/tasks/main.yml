---
- name: kubernetes multimasters
  block:

  - name: Get the kernel revision
    shell: "uname -r"
    register: kernel
    changed_when: False

  - name: Install kernel image
    apt:
      state: present
      name: "{{ 'linux-image-' + kernel.stdout | regex_replace('^([0-9]+\\.[0-9]+\\.[0-9]+)-[0-9]+', '\\1*') }}"
    register: result
    retries: 3
    until: result is success

  - name: modprobe
    modprobe:
      name: "{{ item }}"
      state: present
    with_items:
    - ip_vs
    - nf_conntrack_ipv4

  - name: /etc/modules dm_thin_pool
    lineinfile:
      path: /etc/modules
      line: "{{ item }}"
    with_items:
      - ip_vs
      - nf_conntrack_ipv4

  - name: Fixing nasty bug
    shell: "iptables -P FORWARD ACCEPT"
    ignore_errors: True

  # ssh key sharing between first and other masters
  - stat:
      path: /root/.ssh/id_rsa
    register: first_master_key_query
    when:
      - inventory_hostname == groups['masters'][0]

  - name: Creates an ssh key on first master
    shell: ssh-keygen -b 2048 -t rsa -f /root/.ssh/id_rsa -q -N ""
    args:
      creates: /root/.ssh/id_rsa
    when:
      - inventory_hostname == groups['masters'][0]
      - first_master_key_query.stat.exists == False

  - name: Register pub key on first master
    shell: cat /root/.ssh/id_rsa.pub
    register: first_master_key
    when:
      - inventory_hostname == groups['masters'][0]

  - name: Propagate the key to other masters
    authorized_key:
      key: "{{ hostvars[groups['masters'][0]]['first_master_key'].stdout }}"
      user: root
      state: present
    when:
      - "'masters' in group_names"
      - inventory_hostname != groups['masters'][0]

  - name: Creating /etc/keepalived on master nodes
    file:
      path: /etc/keepalived
      state: directory
    when: "'master in group_names'"

  - name: Templating /etc/keepalived/keepalived.conf
    template:
      src: keepalived.conf.j2
      dest: /etc/keepalived/keepalived.conf
    when: "'masters' in group_names"

  - name: Running keepalived container on masters nodes
    docker_container:
      name: keepalived_api
      image: "chmod666/keepalived:latest"
      state: started
      detach: True
      volumes:
        - /etc/keepalived/keepalived.conf:/usr/local/etc/keepalived/keepalived.conf
      capabilities:
        - NET_ADMIN
      network_mode: host
      restart_policy: always
    when: "'masters' in group_names"

  - name: Wait for keepalived to be alive :-)
    shell: 'docker ps | grep chmod666/keepalived | grep "Up"'
    register: result
    until: result.stdout.find("chmod666/keepalived") != -1
    retries: 18
    delay: 10
    # keepalived runs on masters nodes
    when:
      - "'masters' in group_names"

  # at this point everyone should be able to ping the api floating ip
  # add a test here and don't continue until everyone does not ping the api ip

  - name: Adding Kubernetes official gpg key
    apt_key:
      url: "{{ kubernetes_apt_key }}"
      state: present

  - name: Setting Kubernetes repository depending Ubuntu version
    set_fact:
      kubernetes_repository: "deb http://apt.kubernetes.io/ kubernetes-{{ kubernetes_release }} {{ kubernetes_apt_channel }}"

  - name: Checking Kubernetes repostiory
    debug: var=kubernetes_repository

  - name: Adding Kubernetes repository
    apt_repository:
      repo: "{{ kubernetes_repository }}"
      state: present
      filename: 'kubernetes'

  - name: Installing kubernetes core components (kubectl, kubelet ...)
    apt:
      name: "{{ item }}"
      state: latest
    with_items:
      - kubelet
      - kubeadm
      - kubectl
      - kubernetes-cni
    register: result
    retries: 3
    until: result is success

  # generate a new token for the first masters
  - name: Generating a kubeadm token
    shell: "kubeadm token generate"
    register: kubeadm_token
    when:
      - inventory_hostname == groups["masters"][0]

  # generating the kubeadm config file only on master nodes
  - name: Creating kubeadm_config file
    template:
      src: kubeadm-config.j2
      dest: /tmp/kubeadm_config
    when:
      - "'masters' in group_names"

  # KUBELET_EXTRA_ARGS
  - name: Additional configuration
    template:
      src: local-extras.conf.j2
      dest: /etc/systemd/system/kubelet.service.d/90-local-extras.conf
      mode: 0640
    when:
      - "'masters' in group_names"
      - "kubelet_fail_swap_on == False"
    notify:
      - reload systemd
      - restart kubelet

  - meta: flush_handlers

  # we get kubectl output to check if we have to add the node or not
  # errors are ignore in case of the first initialization on the first master
  - name: Getting kubectl outpout
    shell: "kubectl get nodes"
    ignore_errors: True
    when: inventory_hostname == groups["masters"][0]
    register: kubectl_output

  - name: Kubeadm init on first master
    shell: |
      kubeadm init \
        --config /tmp/kubeadm_config
        {%- if kubeadm_ignore_preflight_errors | length > 0 %} \
          --ignore-preflight-errors={{ kubeadm_ignore_preflight_errors }}
        {% endif %}
    register: kubeadm_output
    when:
      # only run this command if this is the fist master
      # and this one hasn't be initialized
      - inventory_hostname == groups["masters"][0]
      - inventory_hostname not in kubectl_output.stdout

  - name: Kubedadm output
    debug: var=kubeadm_output
    when: inventory_hostname == groups["masters"][0]

  - name: Creating .kube file in $HOME
    # this will be needed on all masters nodes
    file:
      path: ~/.kube
      state: directory
    when:
      - "'masters' in group_names"

  - name: Copying /etc/kubernetes/admin.conf to ~/.kube/config
    copy:
      src: /etc/kubernetes/admin.conf
      dest: ~/.kube/config
      remote_src: yes
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Running first kubectl
    shell: "kubectl get nodes"
    register: first_kubectl
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking kubectl
    debug: var=first_kubectl
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking if flannel deploy exists
    shell: "kubectl get deploy --namespace=kube-system flannel"
    ignore_errors: True
    register: cni_deployment
    when: inventory_hostname == groups["masters"][0]

  - name: Configure cni
    shell: "kubectl apply -f {{ cni_manifest }}"
    when:
      - inventory_hostname == groups["masters"][0]
      - cni_deployment.rc != 0

  # at this point ensure that kubedns is running before continue
  - name: Be sure kubedns is running ok
    shell: "kubectl get pods --namespace=kube-system | grep kube-dns"
    register: result
    until: result.stdout.find("3/3") != -1
    retries: 18
    delay: 10
    when: inventory_hostname == groups["masters"][0]

  # the we create the other masters
  - name: Creating /etc/kubernetes to other masters
    file:
      path: /etc/kubernetes
      state: directory
    when:
      - inventory_hostname != groups["masters"][0]

  - name: Distribute /etc/kubernetes/pki to other masters
    synchronize:
      src: /etc/kubernetes/pki/
      dest: /etc/kubernetes/pki
    when:
      # distribute the pki files on masters that are not the first master
      - "'masters' in  group_names"
      - inventory_hostname != groups["masters"][0]
    delegate_to: "{{ groups.masters[0] }}"

  - name: Initializing other masters
    shell: |
      kubeadm init \
        --config /tmp/kubeadm_config
        {%- if kubeadm_ignore_preflight_errors | length > 0 %} \
          --ignore-preflight-errors={{ kubeadm_ignore_preflight_errors }}
        {% endif %}
    when:
      # this is the master host
      - "'masters' in group_names"
      # this is not the first master
      - inventory_hostname != groups['masters'][0]
      # master hasn't be already initialized
      - inventory_hostname not in hostvars[groups['masters'][0]]['kubectl_output'].stdout

  # fixing kubectl
  - name: kubectl config
    copy:
      src: /etc/kubernetes/kubelet.conf
      dest: /root/.kube/config
      remote_src: True
    when:
      - "'masters' in group_names"
      - inventory_hostname != groups["masters"][0]

  - name: Getting token
    shell: "kubeadm token list | tail -f | awk '{print $1}'"
    register: token
    when:
      - inventory_hostname == groups["masters"][0]

  - name: Checking if kube-proxy is Running
    shell: "ps -ef | grep [k]ube-proxy"
    register: kube_proxy_running
    ignore_errors: True
    when: inventory_hostname != groups["masters"]

  - name: Joining cluster on other nodes
    shell: |
      kubeadm join \
        --token={{ hostvars[groups['masters'][0]]['kubeadm_token'].stdout }} \
        --discovery-token-unsafe-skip-ca-verification \
        {%- if kubeadm_ignore_preflight_errors | length > 0 %}
          --ignore-preflight-errors={{ kubeadm_ignore_preflight_errors }} \
        {% endif %}
        {{ api_floating_ip }}:{{ api_floating_port }}
    register: kubedadm_output
    when:
     - "'masters' not in group_names"
     - "'/usr/local/bin/kube-proxy' not in kube_proxy_running.stdout"

  - name: Kubedadm output
    debug: var=kubeadm_outpout
    when:
     - "'masters' not in group_names"
     - "'/usr/local/bin/kube-proxy' not in kube_proxy_running.stdout"

  # remove this wait and had a test to check all nodes are ready
  - name: Wait for all nodes to be ready
    shell: "kubectl get nodes"
    register: result
    until: result.stdout.find("NotReady") == -1
    retries: 200
    delay: 10
    when:
     - inventory_hostname == groups['masters'][0]

  when:
    - ansible_distribution == "Ubuntu"
    - groups.masters|length > 1
